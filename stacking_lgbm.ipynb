{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8fwQsKV3jA_"
   },
   "source": [
    "References:\n",
    "\n",
    "\n",
    "*   https://www.kaggle.com/code/nawidsayed/lightgbm-and-cnn-3rd-place-solution/notebook\n",
    "*   https://www.kaggle.com/code/mks2192/list-of-fake-samples-and-public-private-lb-split/notebook\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMSjGosA99Rv"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1qL6p2UF99Rv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "import scipy.ndimage\n",
    "from sklearn import linear_model\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WPcd_O_1X-DX"
   },
   "outputs": [],
   "source": [
    "RAND_STATE = 47\n",
    "np.random.seed(RAND_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wnp-xLsba8a5"
   },
   "outputs": [],
   "source": [
    "folder = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFmi0XJbxtOj",
    "outputId": "288c2835-018d-4b78-d77b-71dde1defc27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p70RGcubYzWB"
   },
   "source": [
    "##### Get Updated clean and split files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_Z6V7pKh-2gh"
   },
   "outputs": [],
   "source": [
    "def get_train_test(tr = 'X_train.csv', te = 'X_test_all.csv'):\n",
    "    # Returns a sample of original dataset\n",
    "    # of 100k records of class 0 and all records of class 1\n",
    "    # along with val and test datasets\n",
    "    df = pd.read_csv(folder + tr)\n",
    "    # test_df = pd.read_csv(folder + 'X_test.csv')\n",
    "    test_all_df = pd.read_csv(folder + te)\n",
    "    # Splitting for train and val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df.iloc[:, 3:],\n",
    "                                                    df.target,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=RAND_STATE,\n",
    "                                                    stratify = df['target'])\n",
    "    X_test_all = test_all_df.iloc[:, 3:]\n",
    "    \n",
    "    class_dict = {0 : 100000,\n",
    "                  1 :  y_train.value_counts()[1]}\n",
    "    \n",
    "    rus = RandomUnderSampler(sampling_strategy = class_dict,\n",
    "                             random_state = RAND_STATE) \n",
    "    X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "    X_res.to_csv(folder+'charan/X_res.csv')\n",
    "    y_res.to_csv(folder+'charan/y_res.csv')\n",
    "    \n",
    "    del df, test_all_df\n",
    "#   return X_train, X_val, y_train, y_val, X_test_all\n",
    "    return X_res, X_val, y_res, y_val, X_test_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drC2k-Hgq6c8"
   },
   "source": [
    "#### Initializing models for stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sedKvixaq6c9"
   },
   "outputs": [],
   "source": [
    "def get_stacked_model():\n",
    "    # Logistic Regression\n",
    "    lr_stack = linear_model.LogisticRegression(C=0.1, \n",
    "                                            class_weight='balanced', \n",
    "                                            penalty='l1', \n",
    "                                            solver= 'saga', \n",
    "                                            random_state=RAND_STATE,\n",
    "                                            verbose = 1)\n",
    "    \n",
    "\n",
    "    # Random Forest\n",
    "#     rf_stack = RandomForestClassifier(max_features='auto', \n",
    "#                                     max_depth=15, \n",
    "#                                     min_samples_split=15,\n",
    "#                                     class_weight='balanced', \n",
    "#                                     bootstrap=True ,\n",
    "#                                     random_state=RAND_STATE,\n",
    "#                                     verbose = 1)\n",
    "    # XGBoost Classifier\n",
    "    xgb_stack = XGBClassifier(max_depth=5, \n",
    "                        n_estimators=200,\n",
    "                        random_state=RAND_STATE,\n",
    "                        verbosity = 1,\n",
    "                        scale_pos_weight = 8,\n",
    "                        learning_rate = .05)\n",
    "    \n",
    "    lgbm_stack = LGBMClassifier(bagging_fraction=0.331, bagging_freq=5,\n",
    "                                boost='gbdt', boost_from_average='false',\n",
    "                                class_weight='balanced', feature_fraction=0.045,\n",
    "                                learning_rate=0.0083, metric='auc',\n",
    "                                min_data_in_leaf=80,\n",
    "                                min_sum_hessian_in_leaf=10.0, num_leaves=13,\n",
    "                                num_threads=8, objective='binary',\n",
    "                                random_state=RAND_STATE,\n",
    "                                tree_learner='serial', verbosity=1)\n",
    "\n",
    "    # Initializing the base layer of models\n",
    "    base_estimators = [('xgb', lgbm_stack),\n",
    "                       ('lr', lr_stack)]\n",
    "\n",
    "    # Initiating the stacking classifier using xgboost as the final estimator\n",
    "    model = StackingClassifier(estimators=base_estimators,\n",
    "                               final_estimator=xgb_stack,\n",
    "                               passthrough = True,\n",
    "                               verbose = 1,\n",
    "                               n_jobs = 3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cybD773m0huZ"
   },
   "source": [
    "##### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHcY-7EkrxV6"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, X_test_all = get_train_test()\n",
    "# X_train = pd.read_csv(folder + 'charan/X_res.csv')\n",
    "# y_train = pd.read_csv(folder + 'charan/y_res.csv')\n",
    "# X_val = pd.read_csv(folder+'charan/X_val.csv', index_col=0)\n",
    "# y_val = pd.read_csv(folder+'charan/y_val.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o1MX_NiKr7bK"
   },
   "outputs": [],
   "source": [
    "model = get_stacked_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj6XaXqzi2ii",
    "outputId": "4deb44c3-c770-4ce9-c9c4-99bfe98611f6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/charan/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.38976692\n",
      "Epoch 3, change: 0.24217417\n",
      "Epoch 4, change: 0.17689428\n",
      "Epoch 5, change: 0.14537652\n",
      "Epoch 6, change: 0.11813794\n",
      "Epoch 7, change: 0.10071531\n",
      "Epoch 8, change: 0.08944758\n",
      "Epoch 9, change: 0.07820564\n",
      "Epoch 10, change: 0.07022125\n",
      "Epoch 11, change: 0.06239945\n",
      "Epoch 12, change: 0.05643258\n",
      "Epoch 13, change: 0.05154705\n",
      "Epoch 14, change: 0.04713413\n",
      "Epoch 15, change: 0.04362312\n",
      "Epoch 16, change: 0.04062441\n",
      "Epoch 17, change: 0.03776488\n",
      "Epoch 18, change: 0.03506185\n",
      "Epoch 19, change: 0.03284459\n",
      "Epoch 20, change: 0.03078487\n",
      "Epoch 21, change: 0.02916804\n",
      "Epoch 22, change: 0.02753647\n",
      "Epoch 23, change: 0.02624520\n",
      "Epoch 24, change: 0.02495647\n",
      "Epoch 25, change: 0.02374986\n",
      "Epoch 26, change: 0.02274656\n",
      "Epoch 27, change: 0.02161439\n",
      "Epoch 28, change: 0.02076872\n",
      "Epoch 29, change: 0.01983106\n",
      "Epoch 30, change: 0.01910933\n",
      "Epoch 31, change: 0.01830364\n",
      "Epoch 32, change: 0.01763178\n",
      "Epoch 33, change: 0.01694360\n",
      "Epoch 34, change: 0.01632976\n",
      "Epoch 35, change: 0.01576351\n",
      "Epoch 36, change: 0.01515244\n",
      "Epoch 37, change: 0.01465499\n",
      "Epoch 38, change: 0.01416350\n",
      "Epoch 39, change: 0.01367028\n",
      "Epoch 40, change: 0.01326919\n",
      "Epoch 41, change: 0.01284669\n",
      "Epoch 42, change: 0.01239484\n",
      "Epoch 43, change: 0.01203178\n",
      "Epoch 44, change: 0.01168650\n",
      "Epoch 45, change: 0.01129608\n",
      "Epoch 46, change: 0.01100676\n",
      "Epoch 47, change: 0.01067785\n",
      "Epoch 48, change: 0.01039398\n",
      "Epoch 49, change: 0.01008745\n",
      "Epoch 50, change: 0.00982473\n",
      "Epoch 51, change: 0.00953891\n",
      "Epoch 52, change: 0.00928938\n",
      "Epoch 53, change: 0.00904015\n",
      "Epoch 54, change: 0.00880047\n",
      "Epoch 55, change: 0.00857973\n",
      "Epoch 56, change: 0.00839060\n",
      "Epoch 57, change: 0.00816622\n",
      "Epoch 58, change: 0.00797060\n",
      "Epoch 59, change: 0.00777156\n",
      "Epoch 60, change: 0.00759628\n",
      "Epoch 61, change: 0.00741285\n",
      "Epoch 62, change: 0.00723673\n",
      "Epoch 63, change: 0.00705883\n",
      "Epoch 64, change: 0.00691677\n",
      "Epoch 65, change: 0.00674786\n",
      "Epoch 66, change: 0.00660826\n",
      "Epoch 67, change: 0.00646920\n",
      "Epoch 68, change: 0.00634644\n",
      "Epoch 69, change: 0.00620179\n",
      "Epoch 70, change: 0.00608459\n",
      "Epoch 71, change: 0.00596195\n",
      "Epoch 72, change: 0.00584398\n",
      "Epoch 73, change: 0.00573044\n",
      "Epoch 74, change: 0.00562627\n",
      "Epoch 75, change: 0.00551093\n",
      "Epoch 76, change: 0.00540424\n",
      "Epoch 77, change: 0.00530989\n",
      "Epoch 78, change: 0.00520643\n",
      "Epoch 79, change: 0.00511077\n",
      "Epoch 80, change: 0.00501434\n",
      "Epoch 81, change: 0.00491987\n",
      "Epoch 82, change: 0.00484252\n",
      "Epoch 83, change: 0.00474939\n",
      "Epoch 84, change: 0.00466907\n",
      "Epoch 85, change: 0.00459513\n",
      "Epoch 86, change: 0.00449672\n",
      "Epoch 87, change: 0.00442131\n",
      "Epoch 88, change: 0.00434812\n",
      "Epoch 89, change: 0.00427229\n",
      "Epoch 90, change: 0.00420413\n",
      "Epoch 91, change: 0.00413275\n",
      "Epoch 92, change: 0.00406098\n",
      "Epoch 93, change: 0.00400163\n",
      "Epoch 94, change: 0.00393261\n",
      "Epoch 95, change: 0.00387659\n",
      "Epoch 96, change: 0.00380723\n",
      "Epoch 97, change: 0.00374266\n",
      "Epoch 98, change: 0.00368088\n",
      "Epoch 99, change: 0.00362140\n",
      "Epoch 100, change: 0.00356739\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12862, number of negative: 80000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.521461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157138\n",
      "[LightGBM] [Info] Number of data points in the train set: 92862, number of used features: 800\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12862, number of negative: 80000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.503728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157136\n",
      "[LightGBM] [Info] Number of data points in the train set: 92862, number of used features: 800\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12862, number of negative: 80000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157136\n",
      "[LightGBM] [Info] Number of data points in the train set: 92862, number of used features: 800\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12863, number of negative: 80000\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 157138\n",
      "[LightGBM] [Info] Number of data points in the train set: 92863, number of used features: 800\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12863, number of negative: 80000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157136\n",
      "[LightGBM] [Info] Number of data points in the train set: 92863, number of used features: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charan/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/charan/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min finished\n",
      "/Users/charan/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/charan/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min finished\n",
      "/Users/charan/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   56.5s finished\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 16078, number of negative: 100000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157144\n",
      "[LightGBM] [Info] Number of data points in the train set: 116078, number of used features: 800\n",
      "max_iter reached after 127 seconds\n",
      "max_iter reached after 117 seconds\n",
      "max_iter reached after 117 seconds\n",
      "max_iter reached after 61 seconds\n",
      "max_iter reached after 57 seconds\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.40218852\n",
      "Epoch 3, change: 0.21960630\n",
      "Epoch 4, change: 0.17677894\n",
      "Epoch 5, change: 0.15241119\n",
      "Epoch 6, change: 0.12710400\n",
      "Epoch 7, change: 0.10714369\n",
      "Epoch 8, change: 0.09236497\n",
      "Epoch 9, change: 0.08241769\n",
      "Epoch 10, change: 0.07329043\n",
      "Epoch 11, change: 0.06602124\n",
      "Epoch 12, change: 0.05978028\n",
      "Epoch 13, change: 0.05502413\n",
      "Epoch 14, change: 0.05100779\n",
      "Epoch 15, change: 0.04670873\n",
      "Epoch 16, change: 0.04342917\n",
      "Epoch 17, change: 0.04055136\n",
      "Epoch 18, change: 0.03779307\n",
      "Epoch 19, change: 0.03536404\n",
      "Epoch 20, change: 0.03312703\n",
      "Epoch 21, change: 0.03132415\n",
      "Epoch 22, change: 0.02954732\n",
      "Epoch 23, change: 0.02797292\n",
      "Epoch 24, change: 0.02645800\n",
      "Epoch 25, change: 0.02521516\n",
      "Epoch 26, change: 0.02407963\n",
      "Epoch 27, change: 0.02290195\n",
      "Epoch 28, change: 0.02186689\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 29, change: 0.02102735\n",
      "Epoch 2, change: 0.41014954\n",
      "Epoch 2, change: 0.43230738\n",
      "Epoch 30, change: 0.02018924\n",
      "Epoch 3, change: 0.24142500\n",
      "Epoch 3, change: 0.22924753\n",
      "Epoch 31, change: 0.01942897\n",
      "Epoch 4, change: 0.18395638\n",
      "Epoch 4, change: 0.18014674\n",
      "Epoch 32, change: 0.01875957\n",
      "Epoch 5, change: 0.15204295\n",
      "Epoch 5, change: 0.15252666\n",
      "Epoch 33, change: 0.01800578\n",
      "Epoch 6, change: 0.12471844\n",
      "Epoch 6, change: 0.12739405\n",
      "Epoch 34, change: 0.01737905\n",
      "Epoch 7, change: 0.10458844\n",
      "Epoch 7, change: 0.10521888\n",
      "Epoch 35, change: 0.01681570\n",
      "Epoch 8, change: 0.09226636\n",
      "Epoch 8, change: 0.09299263\n",
      "Epoch 36, change: 0.01623671\n",
      "Epoch 9, change: 0.08183460\n",
      "Epoch 9, change: 0.08345262\n",
      "Epoch 37, change: 0.01566893\n",
      "Epoch 10, change: 0.07207113\n",
      "Epoch 10, change: 0.07352082\n",
      "Epoch 11, change: 0.06508236\n",
      "Epoch 38, change: 0.01518462\n",
      "Epoch 11, change: 0.06641771\n",
      "Epoch 12, change: 0.05950677\n",
      "Epoch 12, change: 0.06051694\n",
      "Epoch 39, change: 0.01467341\n",
      "Epoch 13, change: 0.05413740\n",
      "Epoch 13, change: 0.05559767\n",
      "Epoch 40, change: 0.01419388\n",
      "Epoch 14, change: 0.05029096\n",
      "Epoch 14, change: 0.05109525\n",
      "Epoch 41, change: 0.01381081\n",
      "Epoch 15, change: 0.04603330\n",
      "Epoch 15, change: 0.04709935\n",
      "Epoch 42, change: 0.01341375\n",
      "Epoch 16, change: 0.04291119\n",
      "Epoch 16, change: 0.04355338\n",
      "Epoch 43, change: 0.01301276\n",
      "Epoch 17, change: 0.04015187\n",
      "Epoch 17, change: 0.04066648\n",
      "Epoch 44, change: 0.01262548\n",
      "Epoch 18, change: 0.03763994\n",
      "Epoch 18, change: 0.03798668\n",
      "Epoch 45, change: 0.01227952\n",
      "Epoch 19, change: 0.03540083\n",
      "Epoch 19, change: 0.03556336\n",
      "Epoch 46, change: 0.01195273\n",
      "Epoch 20, change: 0.03351083\n",
      "Epoch 20, change: 0.03351036\n",
      "Epoch 47, change: 0.01160571\n",
      "Epoch 21, change: 0.03172289\n",
      "Epoch 21, change: 0.03173760\n",
      "Epoch 48, change: 0.01130971\n",
      "Epoch 22, change: 0.03013170\n",
      "Epoch 22, change: 0.02997455\n",
      "Epoch 49, change: 0.01101627\n",
      "Epoch 23, change: 0.02860602\n",
      "Epoch 23, change: 0.02852209\n",
      "Epoch 50, change: 0.01072961\n",
      "Epoch 24, change: 0.02722639\n",
      "Epoch 24, change: 0.02714219\n",
      "Epoch 51, change: 0.01044025\n",
      "Epoch 25, change: 0.02609009\n",
      "Epoch 25, change: 0.02597254\n",
      "Epoch 52, change: 0.01018927\n",
      "Epoch 26, change: 0.02488639\n",
      "Epoch 26, change: 0.02470183\n",
      "Epoch 53, change: 0.00992845\n",
      "Epoch 27, change: 0.02388284\n",
      "Epoch 27, change: 0.02364988\n",
      "Epoch 54, change: 0.00967902\n",
      "Epoch 28, change: 0.02288755\n",
      "Epoch 28, change: 0.02260497\n",
      "Epoch 29, change: 0.02193306\n",
      "Epoch 55, change: 0.00945463\n",
      "Epoch 29, change: 0.02171039\n",
      "Epoch 30, change: 0.02109145\n",
      "Epoch 56, change: 0.00921601\n",
      "Epoch 30, change: 0.02085714\n",
      "Epoch 31, change: 0.02030015\n",
      "Epoch 57, change: 0.00901264\n",
      "Epoch 31, change: 0.02004880\n",
      "Epoch 32, change: 0.01959192\n",
      "Epoch 58, change: 0.00880485\n",
      "Epoch 32, change: 0.01929494\n",
      "Epoch 33, change: 0.01881050\n",
      "Epoch 59, change: 0.00859423\n",
      "Epoch 33, change: 0.01856668\n",
      "Epoch 34, change: 0.01817504\n",
      "Epoch 34, change: 0.01792053\n",
      "Epoch 60, change: 0.00841024\n",
      "Epoch 35, change: 0.01756614\n",
      "Epoch 35, change: 0.01734233\n",
      "Epoch 61, change: 0.00824326\n",
      "Epoch 36, change: 0.01697153\n",
      "Epoch 36, change: 0.01672761\n",
      "Epoch 62, change: 0.00804352\n",
      "Epoch 37, change: 0.01642083\n",
      "Epoch 37, change: 0.01614008\n",
      "Epoch 63, change: 0.00786822\n",
      "Epoch 38, change: 0.01588341\n",
      "Epoch 38, change: 0.01563697\n",
      "Epoch 64, change: 0.00770169\n",
      "Epoch 39, change: 0.01534066\n",
      "Epoch 39, change: 0.01511172\n",
      "Epoch 65, change: 0.00753156\n",
      "Epoch 40, change: 0.01488023\n",
      "Epoch 40, change: 0.01463361\n",
      "Epoch 66, change: 0.00737028\n",
      "Epoch 41, change: 0.01445358\n",
      "Epoch 41, change: 0.01416492\n",
      "Epoch 67, change: 0.00721772\n",
      "Epoch 42, change: 0.01405618\n",
      "Epoch 42, change: 0.01377402\n",
      "Epoch 68, change: 0.00707309\n",
      "Epoch 43, change: 0.01364121\n",
      "Epoch 43, change: 0.01335035\n",
      "Epoch 69, change: 0.00693575\n",
      "Epoch 44, change: 0.01322357\n",
      "Epoch 44, change: 0.01296545\n",
      "Epoch 70, change: 0.00679460\n",
      "Epoch 45, change: 0.01286079\n",
      "Epoch 45, change: 0.01259595\n",
      "Epoch 71, change: 0.00665810\n",
      "Epoch 46, change: 0.01251380\n",
      "Epoch 46, change: 0.01225153\n",
      "Epoch 72, change: 0.00653989\n",
      "Epoch 47, change: 0.01216639\n",
      "Epoch 47, change: 0.01189635\n",
      "Epoch 73, change: 0.00640669\n",
      "Epoch 48, change: 0.01182975\n",
      "Epoch 48, change: 0.01159533\n",
      "Epoch 74, change: 0.00628552\n",
      "Epoch 49, change: 0.01149870\n",
      "Epoch 49, change: 0.01127650\n",
      "Epoch 75, change: 0.00616414\n",
      "Epoch 50, change: 0.01120193\n",
      "Epoch 50, change: 0.01097016\n",
      "Epoch 76, change: 0.00604699\n",
      "Epoch 51, change: 0.01088507\n",
      "Epoch 51, change: 0.01069187\n",
      "Epoch 77, change: 0.00593960\n",
      "Epoch 52, change: 0.01061569\n",
      "Epoch 52, change: 0.01041937\n",
      "Epoch 78, change: 0.00583333\n",
      "Epoch 53, change: 0.01032157\n",
      "Epoch 53, change: 0.01016624\n",
      "Epoch 79, change: 0.00571721\n",
      "Epoch 54, change: 0.01004475\n",
      "Epoch 54, change: 0.00988460\n",
      "Epoch 80, change: 0.00561756\n",
      "Epoch 55, change: 0.00979142\n",
      "Epoch 55, change: 0.00966408\n",
      "Epoch 81, change: 0.00550917\n",
      "Epoch 56, change: 0.00955199\n",
      "Epoch 56, change: 0.00942625\n",
      "Epoch 82, change: 0.00540667\n",
      "Epoch 57, change: 0.00931801\n",
      "Epoch 57, change: 0.00920584\n",
      "Epoch 83, change: 0.00531970\n",
      "Epoch 58, change: 0.00908539\n",
      "Epoch 58, change: 0.00898133\n",
      "Epoch 84, change: 0.00522959\n",
      "Epoch 59, change: 0.00886435\n",
      "Epoch 59, change: 0.00876795\n",
      "Epoch 85, change: 0.00512952\n",
      "Epoch 60, change: 0.00865943\n",
      "Epoch 60, change: 0.00855637\n",
      "Epoch 86, change: 0.00504693\n",
      "Epoch 61, change: 0.00848150\n",
      "Epoch 61, change: 0.00837512\n",
      "Epoch 87, change: 0.00496028\n",
      "Epoch 62, change: 0.00826892\n",
      "Epoch 62, change: 0.00816426\n",
      "Epoch 88, change: 0.00487979\n",
      "Epoch 63, change: 0.00807500\n",
      "Epoch 63, change: 0.00796925\n",
      "Epoch 89, change: 0.00479987\n",
      "Epoch 64, change: 0.00790340\n",
      "Epoch 64, change: 0.00780163\n",
      "Epoch 90, change: 0.00471590\n",
      "Epoch 65, change: 0.00772989\n",
      "Epoch 65, change: 0.00762674\n",
      "Epoch 91, change: 0.00464470\n",
      "Epoch 66, change: 0.00755740\n",
      "Epoch 66, change: 0.00744650\n",
      "Epoch 92, change: 0.00455596\n",
      "Epoch 67, change: 0.00739185\n",
      "Epoch 67, change: 0.00729021\n",
      "Epoch 93, change: 0.00448317\n",
      "Epoch 68, change: 0.00723742\n",
      "Epoch 68, change: 0.00713227\n",
      "Epoch 94, change: 0.00441035\n",
      "Epoch 69, change: 0.00708216\n",
      "Epoch 69, change: 0.00698512\n",
      "Epoch 95, change: 0.00434354\n",
      "Epoch 70, change: 0.00692765\n",
      "Epoch 70, change: 0.00683338\n",
      "Epoch 96, change: 0.00426861\n",
      "Epoch 71, change: 0.00678907\n",
      "Epoch 71, change: 0.00669389\n",
      "Epoch 97, change: 0.00420643\n",
      "Epoch 72, change: 0.00665677\n",
      "Epoch 72, change: 0.00656418\n",
      "Epoch 98, change: 0.00413957\n",
      "Epoch 73, change: 0.00651950\n",
      "Epoch 73, change: 0.00642558\n",
      "Epoch 99, change: 0.00407567\n",
      "Epoch 74, change: 0.00639945\n",
      "Epoch 74, change: 0.00629833\n",
      "Epoch 100, change: 0.00401796\n",
      "Epoch 75, change: 0.00626890\n",
      "Epoch 75, change: 0.00616992\n",
      "Epoch 76, change: 0.00615033\n",
      "Epoch 76, change: 0.00605400\n",
      "Epoch 77, change: 0.00602786\n",
      "Epoch 77, change: 0.00593765\n",
      "Epoch 78, change: 0.00591734\n",
      "Epoch 78, change: 0.00582866\n",
      "Epoch 79, change: 0.00573398\n",
      "Epoch 79, change: 0.00579504\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 80, change: 0.00562845\n",
      "Epoch 80, change: 0.00568875\n",
      "Epoch 2, change: 0.33636445\n",
      "Epoch 81, change: 0.00552499\n",
      "Epoch 3, change: 0.21576879\n",
      "Epoch 81, change: 0.00557455\n",
      "Epoch 4, change: 0.17458292\n",
      "Epoch 82, change: 0.00542872\n",
      "Epoch 82, change: 0.00547643\n",
      "Epoch 5, change: 0.14713194\n",
      "Epoch 83, change: 0.00533814\n",
      "Epoch 83, change: 0.00537650\n",
      "Epoch 6, change: 0.12535879\n",
      "Epoch 84, change: 0.00524698\n",
      "Epoch 84, change: 0.00528389\n",
      "Epoch 7, change: 0.10792445\n",
      "Epoch 85, change: 0.00515658\n",
      "Epoch 85, change: 0.00517980\n",
      "Epoch 8, change: 0.09322066\n",
      "Epoch 86, change: 0.00507001\n",
      "Epoch 86, change: 0.00509507\n",
      "Epoch 9, change: 0.08051656\n",
      "Epoch 87, change: 0.00498752\n",
      "Epoch 87, change: 0.00499989\n",
      "Epoch 10, change: 0.07369968\n",
      "Epoch 88, change: 0.00490288\n",
      "Epoch 88, change: 0.00491766\n",
      "Epoch 11, change: 0.06594979\n",
      "Epoch 89, change: 0.00483436\n",
      "Epoch 12, change: 0.05998258\n",
      "Epoch 89, change: 0.00483311\n",
      "Epoch 13, change: 0.05435546\n",
      "Epoch 90, change: 0.00475362\n",
      "Epoch 90, change: 0.00474472\n",
      "Epoch 14, change: 0.05000350\n",
      "Epoch 91, change: 0.00467177\n",
      "Epoch 91, change: 0.00466852\n",
      "Epoch 15, change: 0.04649441\n",
      "Epoch 92, change: 0.00459958\n",
      "Epoch 92, change: 0.00458039\n",
      "Epoch 16, change: 0.04297431\n",
      "Epoch 93, change: 0.00452426\n",
      "Epoch 93, change: 0.00450869\n",
      "Epoch 17, change: 0.04007865\n",
      "Epoch 94, change: 0.00445270\n",
      "Epoch 94, change: 0.00442944\n",
      "Epoch 18, change: 0.03752075\n",
      "Epoch 95, change: 0.00438421\n",
      "Epoch 95, change: 0.00435714\n",
      "Epoch 19, change: 0.03500649\n",
      "Epoch 96, change: 0.00431706\n",
      "Epoch 96, change: 0.00428014\n",
      "Epoch 20, change: 0.03297343\n",
      "Epoch 97, change: 0.00425532\n",
      "Epoch 21, change: 0.03096065\n",
      "Epoch 97, change: 0.00421419\n",
      "Epoch 98, change: 0.00419234\n",
      "Epoch 22, change: 0.02937486\n",
      "Epoch 98, change: 0.00414629\n",
      "Epoch 99, change: 0.00413327\n",
      "Epoch 23, change: 0.02780338\n",
      "Epoch 99, change: 0.00408244\n",
      "Epoch 100, change: 0.00407040\n",
      "Epoch 24, change: 0.02634418\n",
      "Epoch 100, change: 0.00401746\n",
      "Epoch 25, change: 0.02494356\n",
      "Epoch 26, change: 0.02385928\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 27, change: 0.02269716\n",
      "Epoch 2, change: 0.36041764\n",
      "Epoch 28, change: 0.02168032\n",
      "Epoch 3, change: 0.22612848\n",
      "Epoch 29, change: 0.02072602\n",
      "Epoch 4, change: 0.18064123\n",
      "Epoch 30, change: 0.01980083\n",
      "Epoch 5, change: 0.14865967\n",
      "Epoch 31, change: 0.01898139\n",
      "Epoch 6, change: 0.12701666\n",
      "Epoch 32, change: 0.01822970\n",
      "Epoch 7, change: 0.10687513\n",
      "Epoch 8, change: 0.09345247\n",
      "Epoch 33, change: 0.01757012\n",
      "Epoch 9, change: 0.08006173\n",
      "Epoch 34, change: 0.01695803\n",
      "Epoch 10, change: 0.07390890\n",
      "Epoch 35, change: 0.01633475\n",
      "Epoch 11, change: 0.06559523\n",
      "Epoch 36, change: 0.01578984\n",
      "Epoch 12, change: 0.05983491\n",
      "Epoch 37, change: 0.01525530\n",
      "Epoch 13, change: 0.05484711\n",
      "Epoch 38, change: 0.01476167\n",
      "Epoch 14, change: 0.05033167\n",
      "Epoch 39, change: 0.01430272\n",
      "Epoch 15, change: 0.04669660\n",
      "Epoch 40, change: 0.01383456\n",
      "Epoch 16, change: 0.04292218\n",
      "Epoch 41, change: 0.01340997\n",
      "Epoch 17, change: 0.04005184\n",
      "Epoch 42, change: 0.01300524\n",
      "Epoch 18, change: 0.03756676\n",
      "Epoch 43, change: 0.01264584\n",
      "Epoch 19, change: 0.03505807\n",
      "Epoch 44, change: 0.01224155\n",
      "Epoch 20, change: 0.03294752\n",
      "Epoch 45, change: 0.01194503\n",
      "Epoch 21, change: 0.03110386\n",
      "Epoch 46, change: 0.01155932\n",
      "Epoch 22, change: 0.02945525\n",
      "Epoch 23, change: 0.02788132\n",
      "Epoch 47, change: 0.01127698\n",
      "Epoch 24, change: 0.02648220\n",
      "Epoch 48, change: 0.01095308\n",
      "Epoch 25, change: 0.02505301\n",
      "Epoch 49, change: 0.01063624\n",
      "Epoch 26, change: 0.02393167\n",
      "Epoch 50, change: 0.01036044\n",
      "Epoch 27, change: 0.02280547\n",
      "Epoch 51, change: 0.01012269\n",
      "Epoch 28, change: 0.02174632\n",
      "Epoch 52, change: 0.00986846\n",
      "Epoch 29, change: 0.02084233\n",
      "Epoch 53, change: 0.00962169\n",
      "Epoch 30, change: 0.01988695\n",
      "Epoch 54, change: 0.00938927\n",
      "Epoch 31, change: 0.01905906\n",
      "Epoch 55, change: 0.00917159\n",
      "Epoch 32, change: 0.01828548\n",
      "Epoch 56, change: 0.00896116\n",
      "Epoch 33, change: 0.01749897\n",
      "Epoch 57, change: 0.00875489\n",
      "Epoch 34, change: 0.01684949\n",
      "Epoch 58, change: 0.00856956\n",
      "Epoch 35, change: 0.01626445\n",
      "Epoch 59, change: 0.00837855\n",
      "Epoch 36, change: 0.01566376\n",
      "Epoch 60, change: 0.00819362\n",
      "Epoch 37, change: 0.01507919\n",
      "Epoch 61, change: 0.00801471\n",
      "Epoch 38, change: 0.01454288\n",
      "Epoch 62, change: 0.00785271\n",
      "Epoch 39, change: 0.01405517\n",
      "Epoch 63, change: 0.00768263\n",
      "Epoch 40, change: 0.01358651\n",
      "Epoch 64, change: 0.00753454\n",
      "Epoch 41, change: 0.01313165\n",
      "Epoch 65, change: 0.00737667\n",
      "Epoch 42, change: 0.01272330\n",
      "Epoch 66, change: 0.00722763\n",
      "Epoch 43, change: 0.01236329\n",
      "Epoch 67, change: 0.00707973\n",
      "Epoch 44, change: 0.01196262\n",
      "Epoch 68, change: 0.00695898\n",
      "Epoch 45, change: 0.01166095\n",
      "Epoch 46, change: 0.01130121\n",
      "Epoch 69, change: 0.00680207\n",
      "Epoch 47, change: 0.01100763\n",
      "Epoch 70, change: 0.00668872\n",
      "Epoch 48, change: 0.01069040\n",
      "Epoch 71, change: 0.00655756\n",
      "Epoch 49, change: 0.01039963\n",
      "Epoch 72, change: 0.00643354\n",
      "Epoch 50, change: 0.01013262\n",
      "Epoch 73, change: 0.00629889\n",
      "Epoch 51, change: 0.00987734\n",
      "Epoch 74, change: 0.00620405\n",
      "Epoch 52, change: 0.00962866\n",
      "Epoch 75, change: 0.00608551\n",
      "Epoch 53, change: 0.00939810\n",
      "Epoch 76, change: 0.00597806\n",
      "Epoch 54, change: 0.00916825\n",
      "Epoch 77, change: 0.00586968\n",
      "Epoch 55, change: 0.00892342\n",
      "Epoch 78, change: 0.00576825\n",
      "Epoch 56, change: 0.00870904\n",
      "Epoch 79, change: 0.00566451\n",
      "Epoch 57, change: 0.00849841\n",
      "Epoch 80, change: 0.00556847\n",
      "Epoch 58, change: 0.00830213\n",
      "Epoch 81, change: 0.00547497\n",
      "Epoch 59, change: 0.00812245\n",
      "Epoch 82, change: 0.00537616\n",
      "Epoch 60, change: 0.00794367\n",
      "Epoch 83, change: 0.00529231\n",
      "Epoch 61, change: 0.00778602\n",
      "Epoch 84, change: 0.00520308\n",
      "Epoch 62, change: 0.00760774\n",
      "Epoch 85, change: 0.00511205\n",
      "Epoch 63, change: 0.00745362\n",
      "Epoch 86, change: 0.00503198\n",
      "Epoch 64, change: 0.00730562\n",
      "Epoch 87, change: 0.00494919\n",
      "Epoch 65, change: 0.00716333\n",
      "Epoch 88, change: 0.00486949\n",
      "Epoch 66, change: 0.00701717\n",
      "Epoch 89, change: 0.00479245\n",
      "Epoch 67, change: 0.00687346\n",
      "Epoch 90, change: 0.00471886\n",
      "Epoch 68, change: 0.00675716\n",
      "Epoch 91, change: 0.00464363\n",
      "Epoch 69, change: 0.00660722\n",
      "Epoch 92, change: 0.00457046\n",
      "Epoch 70, change: 0.00649030\n",
      "Epoch 93, change: 0.00449930\n",
      "Epoch 71, change: 0.00636991\n",
      "Epoch 94, change: 0.00442443\n",
      "Epoch 72, change: 0.00624400\n",
      "Epoch 95, change: 0.00436265\n",
      "Epoch 73, change: 0.00612072\n",
      "Epoch 96, change: 0.00429017\n",
      "Epoch 74, change: 0.00602147\n",
      "Epoch 97, change: 0.00423678\n",
      "Epoch 75, change: 0.00591437\n",
      "Epoch 98, change: 0.00417339\n",
      "Epoch 76, change: 0.00580584\n",
      "Epoch 99, change: 0.00410172\n",
      "Epoch 77, change: 0.00569088\n",
      "Epoch 100, change: 0.00404519\n",
      "Epoch 78, change: 0.00560346\n",
      "Epoch 79, change: 0.00549786\n",
      "Epoch 80, change: 0.00541277\n",
      "Epoch 81, change: 0.00531568\n",
      "Epoch 82, change: 0.00521900\n",
      "Epoch 83, change: 0.00513873\n",
      "Epoch 84, change: 0.00505085\n",
      "Epoch 85, change: 0.00496726\n",
      "Epoch 86, change: 0.00488862\n",
      "Epoch 87, change: 0.00481333\n",
      "Epoch 88, change: 0.00473431\n",
      "Epoch 89, change: 0.00465394\n",
      "Epoch 90, change: 0.00458771\n",
      "Epoch 91, change: 0.00451689\n",
      "Epoch 92, change: 0.00444584\n",
      "Epoch 93, change: 0.00437815\n",
      "Epoch 94, change: 0.00430450\n",
      "Epoch 95, change: 0.00424390\n",
      "Epoch 96, change: 0.00417495\n",
      "Epoch 97, change: 0.00412429\n",
      "Epoch 98, change: 0.00406083\n",
      "Epoch 99, change: 0.00399432\n",
      "Epoch 100, change: 0.00393966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('lr',\n",
       "                                LogisticRegression(C=0.1,\n",
       "                                                   class_weight='balanced',\n",
       "                                                   penalty='l1',\n",
       "                                                   random_state=47,\n",
       "                                                   solver='saga', verbose=1)),\n",
       "                               ('lgbm',\n",
       "                                LGBMClassifier(bagging_fraction=0.331,\n",
       "                                               bagging_freq=5, boost='gbdt',\n",
       "                                               boost_from_average='false',\n",
       "                                               class_weight='balanced',\n",
       "                                               feature_fraction=0.045,\n",
       "                                               learning_rate=0.0083,\n",
       "                                               metric='auc',\n",
       "                                               min_data_in_leaf=80,\n",
       "                                               min_sum...\n",
       "                                                 importance_type=None,\n",
       "                                                 interaction_constraints=None,\n",
       "                                                 learning_rate=0.05,\n",
       "                                                 max_bin=None,\n",
       "                                                 max_cat_to_onehot=None,\n",
       "                                                 max_delta_step=None,\n",
       "                                                 max_depth=5, max_leaves=None,\n",
       "                                                 min_child_weight=None,\n",
       "                                                 missing=nan,\n",
       "                                                 monotone_constraints=None,\n",
       "                                                 n_estimators=200, n_jobs=None,\n",
       "                                                 num_parallel_tree=None,\n",
       "                                                 predictor=None,\n",
       "                                                 random_state=47,\n",
       "                                                 reg_alpha=None,\n",
       "                                                 reg_lambda=None, ...),\n",
       "                   n_jobs=3, passthrough=True, verbose=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qGu217f0-PV"
   },
   "source": [
    "##### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiOBF9CA0_2S",
    "outputId": "251841bc-4af1-41cf-ca11-7b1903a96de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AuC: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8113423732317924"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training AuC: \\n')\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = pd.read_csv(folder+'charan/X_val.csv', index_col=0)\n",
    "# y_val = pd.read_csv(folder+'charan/y_val.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gBFfGX4b1PZg",
    "outputId": "595c687e-a3f5-43bc-b8ac-4f745d41d7f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuC: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7959"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Validation AuC: \\n')\n",
    "model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vII_Z2ds1X34"
   },
   "source": [
    "##### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0tg-iA6D1W03"
   },
   "outputs": [],
   "source": [
    "# save\n",
    "pkl.dump(model, open(folder + 'charan/' + \n",
    "                     'lgbm_lr_xgb_stack_20220420.pkl.dat', \n",
    "                     \"wb\"))\n",
    "# load\n",
    "# xgb_stack_loaded = pkl.load(open(folder + \"charan/\" +\n",
    "#lgbm_lr_xgb_stack_20220420.pkl.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 16078, number of negative: 100000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157144\n",
      "[LightGBM] [Info] Number of data points in the train set: 116078, number of used features: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12862, number of negative: 80000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.339283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157138\n",
      "[LightGBM] [Info] Number of data points in the train set: 92862, number of used features: 800\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12862, number of negative: 80000\n",
      "[LightGBM] [Info] Number of positive: 12862, number of negative: 80000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.788855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.795057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157136\n",
      "[LightGBM] [Info] Total Bins 157136\n",
      "[LightGBM] [Info] Number of data points in the train set: 92862, number of used features: 800\n",
      "[LightGBM] [Info] Number of data points in the train set: 92862, number of used features: 800\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12863, number of negative: 80000\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.382613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157136\n",
      "[LightGBM] [Info] Number of data points in the train set: 92863, number of used features: 800\n",
      "[LightGBM] [Warning] boosting is set with boost=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10.0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10.0\n",
      "[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] feature_fraction is set=0.045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.331\n",
      "[LightGBM] [Info] Number of positive: 12863, number of negative: 80000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 157138\n",
      "[LightGBM] [Info] Number of data points in the train set: 92863, number of used features: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed: 53.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  3.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AuC: \n",
      " 0.9426075569875428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuC: \n",
      " 0.87305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "model2 = get_stacked_model()\n",
    "model2.fit(X_train, y_train)\n",
    "print('Training AuC: \\n', \n",
    "      model2.score(X_train, y_train))\n",
    "print('Validation AuC: \\n', \n",
    "      model2.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(model, open(folder + \"charan/lgbm_xgb_rf_stack_20220420.pkl.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_submission(model, X_test_all):\n",
    "    y_pred = model.predict_proba(X_test_all)\n",
    "    kaggle = pd.DataFrame({\n",
    "        'ID_code': X_test_all.index.map(lambda x: 'test_{}'.format(x)),\n",
    "        'target': y_pred[:,1]})\n",
    "    kaggle = kaggle.set_index('ID_code')\n",
    "    return kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bzZhbYTva3w"
   },
   "source": [
    "##### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rJhVMDUGd3rc",
    "outputId": "a9847e90-1574-4ae7-88d0-60ba41ebe7b8"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_test_all)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "swqLlZNmeCvk"
   },
   "outputs": [],
   "source": [
    "kaggle = pd.DataFrame({\n",
    "    'ID_code': X_test_all.index.map(lambda x: 'test_{}'.format(x)),\n",
    "    'target': y_pred[:,1]\n",
    "})\n",
    "kaggle = kaggle.set_index('ID_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "QbsNShx_ekYC"
   },
   "outputs": [],
   "source": [
    "kaggle.to_csv(folder + 'charan/kaggle9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs model, X_val and y_val to plot confusion matrix\n",
    "def show_confusion_matrix(model, X_val, y_val):\n",
    "    print('Confusion Matrix')\n",
    "    plot_confusion_matrix(model, X_val, y_val)\n",
    "    plt.show()\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "    print('Accuracy of the model: ',  accuracy_score(y_val, y_pred))\n",
    "    print('Recall of the model: ',    recall_score(y_val, y_pred))\n",
    "    print('Precision of the model: ', precision_score(y_val, y_pred))\n",
    "    print('F Measure of the model: ', f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs model, X_val and y_val to plot RoC curve\n",
    "def plot_roc(model, X_val, y_val):\n",
    "    # calculate the fpr and tpr for \n",
    "    # all thresholds of the classification\n",
    "    preds = model.predict_proba(X_val)[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_val, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # Plot\n",
    "    plt.title('Receiver Operating Characteristic - Stacked Ensemble Model')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([-0.1, 1.1])\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.79      0.87     35980\n",
      "         1.0       0.31      0.81      0.44      4020\n",
      "\n",
      "    accuracy                           0.80     40000\n",
      "   macro avg       0.64      0.80      0.66     40000\n",
      "weighted avg       0.91      0.80      0.83     40000\n",
      "\n",
      "Accuracy of the model:  0.7959\n",
      "Recall of the model:  0.8114427860696517\n",
      "Precision of the model:  0.30577427821522307\n",
      "F Measure of the model:  0.44417211328976025\n"
     ]
    }
   ],
   "source": [
    "show_confusion_matrix(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stacking-sourabh.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
